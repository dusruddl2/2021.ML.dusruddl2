{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이번 강의 목표 \n",
    "\n",
    "* 서포트벡터머신의 이론적 배경  \n",
    "> * 하드마진 서포트벡터머신 분류기 \n",
    "$$ $$\n",
    "> * 소프트마진 서포트벡터머신 분류기 \n",
    "\n",
    "* 서포트벡터머신 분류기의 학습 (`ML2020-09-01-(컨벡스) 최적화 - 원시문제와 쌍대문제`에서 배운 내용을 바탕으로 이해)  \n",
    "> * 원초문제와 쌍대문제 사이의 강한 쌍대성과 KKT  \n",
    "$$ $$\n",
    "> * 서포트벡터머신 분류기에 학습에 대한 최적화 문제를 힌지손실함수에 대한 최적화 문제로 이해 \n",
    "\n",
    "* 비선형 서포트벡터머신과 커널 기법의 이해 \n",
    "> * 비선형 특성을 추가하여 서포트벡터머신을 적용하는 방법의 장,단점\n",
    "$$ $$\n",
    "> * 커널 기법을 이용하는 이유와 효과 \n",
    "$$ $$\n",
    "> * 커널 SVM에서 예측을 하는 방법 \n",
    "\n",
    "* 선형, 비선형 서포트벡터머신 회귀 모델 \n",
    "\n",
    "* `sklearn.svm` 모듈의 해당 클래스 소개 \n",
    "\n",
    "* 선형대수학 내용  \n",
    "> * 내적공간 $\\mathbb R^n$, 내적 \n",
    "$$ $$\n",
    "> * 양의 정부호행렬과 내적의 관계 \n",
    "$$ $$\n",
    "> * 초평면의 방정식 \n",
    "\n",
    "* 읽을거리  \n",
    "> * sklearn의 서포트벡터머신 분류, 회귀 관련 설명 [링크](https://scikit-learn.org/stable/modules/svm.html#svm-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서포트벡터머신 (SVM: Support Vector Machine)\n",
    "\n",
    "* 선형이나 비선형 분류, 회귀, 이상치 탐지 등에 활용할 수 있는 매우 강력한 다목적 머신러닝 모델 \n",
    "\n",
    "\n",
    "* 이 강의에서는 서포트벡터머신 분류기에 대해 이론적 배경을 포함하여 자세하게 살펴보고, sklearn module을 이용하여 분류, 회귀 모델을 사용하는 방법에 대해 설명 \n",
    "\n",
    "* 선형 SVM 분류기 모델   \n",
    "> * 이진분류문제에서 훈련 데이터셋 $\\{(\\mathbf x_i,y_i):1\\le i\\le m\\}$ (단, $\\mathbf x_i \\in \\mathbb R^n,\\, y_i\\in \\mathbb R$)이 주어질 때, <span style=\"color:blue\">적절한 조건을 만족하는</span> 초평면 $\\mathbf w^{\\rm T}\\mathbf x+b=0$를 구해서 다음과 같이 판별하는 모델 \n",
    "$$\n",
    "y = \\begin{cases} 1 & \\ \\text{if } \\mathbf w^{\\rm T}\\mathbf x + b>0\\\\\n",
    "-1 & \\ \\text{if} \\mathbf w^{\\rm T}\\mathbf x + b<0\n",
    "\\end{cases}\n",
    "$$\n",
    "$$ $$\n",
    "> * <span style=\"color:blue\">훈련 데이터셋의 특성벡터 $\\mathbf x_i$들이 선형분리가능한 경우에 적용시킬 수 있는 **하드마진 서포트벡터머신**과 선형분리가능하지 않은 경우에도 적용시킬 수 있는 **소프트마진 서포트벡터머신**이 있음 </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하드마진 선형 SVM 분류모델 \n",
    "\n",
    "* 우선 서포트벡터머신의 기본적인 아이디어를 이해하기 위해, <span style=\"color:red\">선형분리가능한 경우</span>부터 살펴보자. \n",
    "\n",
    "* 훈련 데이터셋이 선형분리가능인 경우에는 적절한 초평면 $f(\\mathbf x)=b+w_1x_1+\\cdots+w_nx_n=0$에 의해 레이블이 다른 샘플들을 완전히 분리시킬 수 있다.  \n",
    "> * 위의 (아핀)함수 $f:\\mathbb R^n\\to \\mathbb R$은 $\\mathbb R^n$의 내적을 이용하면 $f(\\mathbf x)=\\mathbf w^{\\rm T}\\mathbf x+b=\\langle \\mathbf w,\\mathbf x\\rangle +b$로 쓸 수 있음  \n",
    "$$ $$\n",
    "> * $\\mathcal P = \\{\\mathbf x\\in \\mathbb R^n|f(\\mathbf x)=0\\}$라 할 때, $\\mathcal P$를 $\\mathbb R^n$의 초평면이라 하고, $\\mathbf w=(w_1,\\cdots,w_n)$의 초평면에 대한 **법선벡터**라고 함   \n",
    "$$ $$\n",
    "> * $\\mathbf a$와 $\\mathbf b$가 $\\mathcal P$의 원소이면 $0=f(\\mathbf a)-f(\\mathbf b)=\\langle \\mathbf w,\\mathbf a-\\mathbf b\\rangle$이므로 $\\mathbf a-\\mathbf b$가 $\\mathbf w$와 수직인 벡터 \n",
    "\n",
    "<img src=\".\\images\\week11\\svm_c1.png\" width=\"400\">\n",
    "\n",
    "* 이때, $f(\\mathbf x_i)>0$인 $\\mathbf x_i$의 레이블을 $y_i = 1$, $f(\\mathbf x_i)<0$인 $\\mathbf x_i$의 레이블을 $y_i=-1$이라고 하면, 훈련데이터의 모든 샘플 $\\mathbf x_i$에 대해 \n",
    "$$ y_i(\\langle \\mathbf w,\\mathbf x_i\\rangle +b)> 0,\\ (1\\le i \\le m)$$\n",
    "$$ $$\n",
    "\n",
    "* 각 샘플 $\\mathbf x_i$에서 평면까지의 거리를 평면까지의 수직거리라 할 때, 훈련 데이터 각 샘플에서 평면까지의 거리 중 최솟값을 주어진 초평면에 대한 **마진(margin)**이라고 함  \n",
    "(<span style=\"color:blue\">평면까지의 수직 거리: $\\mathbf x_i$에 대해 $\\mathbf x_i-\\tilde{\\mathbf x}_i=t\\mathbf w$가 되도록 $\\tilde{\\mathbf x}_i\\in \\mathcal P$를 잡을 때 노름 $||t\\mathbf w||$</span>, 이때, $\\tilde {\\mathbf x}_i$는 $\\mathbf x_i$를 평면에 내린 수선의 발)\n",
    "$$ $$\n",
    "<img src=\".\\images\\week11\\svm_c2.png\" width=\"400\">\n",
    "* 주어진 훈련 데이터셋의 선형분리가능한 샘플들을 분리시키는 초평면 중에서 <span style=\"color:blue\">마진(margin)이 최대가 되는 초평면(즉, 가중치 $\\mathbf w$와 절편 $b$)으로 분류기를 구성하는 것</span>을 분류문제에 대한 **하드마진(hard margin) 서포트벡터머신** 모델이라고 함   \n",
    "> * 마진이 최대가 되는 초평면의 법선벡터는 방향만이 중요하고 크기는 중요하지 않음\n",
    "$$ $$  \n",
    "> * 따라서 크기가 1($||\\mathbf w||=1$)로 가정해도 되고, 이 경우 평면까지의 거리가 마진이 되는 샘플 벡터 $\\mathbf x_i$에 대해 마진은 $f(\\mathbf x_i)$가 됨.  \n",
    "> * 마진을 결정하는 샘플 벡터를 **서포트벡터(support vector)**라고 함  \n",
    "$$ $$\n",
    "> * 서포트벡터는 레이블이 $1$인 경우와 레이블이 $-1$인 경우에 항상 생김 \n",
    "$$ $$\n",
    "> * 하드마진 서포트벡터머신에서 마진이 최대(최댓값 $r$)가 되는 초평면($\\mathbf w$, $b$)를 찾고 나면 훈련 데이터셋의 모든 샘플에 대해 다음이 성립 \n",
    "$$y_i(\\langle \\mathbf w,\\mathbf x_i\\rangle +b)\\ge r,\\ (1\\le i \\le m)\\, (단, ||\\mathbf w||=1)$$\n",
    "$$ $$\n",
    "> * 따라서, 하드마진 서포트벡터머신의 학습 알고리즘은 다음 최적화 문제로 바뀜 \n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\quad \\max_{\\mathbf w, b, r} r\\\\\n",
    "&\\text{s.t }\\ y_i(\\langle \\mathbf w,\\mathbf x_i\\rangle +b)\\ge r,\\ (1\\le i\\le m),\\ ||\\mathbf w||=1,\\ r>0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* 위 최적화문제를 변형하여 최대마진이 $r$인 서포트벡터 $\\mathbf x$가 항상 $\\langle \\mathbf w,\\mathbf x\\rangle+b=1$을 만족시키도록 $\\mathbf w$와 $b$를 잡는 것으로 바꾸어 생각하면(즉, $||\\mathbf w||=1$ 조건을 없애고), \n",
    "> * $\\mathbf x$를 평면에 내린 수선의 발 $\\tilde{\\mathbf x}$에 대해 $\\mathbf x=\\tilde{\\mathbf x}+r\\dfrac{\\mathbf w}{||\\mathbf w||}$이므로 조건 $\\langle \\mathbf w,\\mathbf x\\rangle+b = 1$에서\n",
    "$$ \n",
    "\\langle \\mathbf w, \\tilde{\\mathbf x}\\rangle+\\dfrac{r}{||\\mathbf w||}\\langle\\mathbf w,\\mathbf w\\rangle+b = 1$$\n",
    "$$ $$\n",
    "> * 한편, $\\tilde{\\mathbf x}$은 초평면 위의 점이므로 $\\langle\\mathbf w,\\tilde{\\mathbf x}\\rangle+b=0$이므로 최대마진은 $r=\\dfrac 1 {||\\mathbf w||}$가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하드마진 선형 SVM 분류의 학습 알고리즘 (선형분리가능인 경우)\n",
    "\n",
    "* 위 설명에 의해 서포트벡터 $\\mathbf x$가 $\\langle \\mathbf w,\\mathbf x\\rangle+b=1$을 만족시키는 것을 가정할 때, 선형 SVM 분류기에 대한 학습 알고리즘은 다음 최적화 문제의 해를 구하는 것으로 변형됨 \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\quad \\min_{\\mathbf w,b} \\dfrac 1 2 ||\\mathbf w||^2\\\\\n",
    "&\\text{s.t }\\ y_i(\\langle \\mathbf w,\\mathbf x_i\\rangle +b)\\ge 1 \\ (1\\le i \\le m)\n",
    "\\end{aligned} \\quad \\cdots (1)\n",
    "$$\n",
    "> * 원래 최적화 문제에서 $r=1/||\\mathbf w||$의 최댓값을 구하는 것은 $||\\mathbf w||$의 최솟값을 구하는 것과 같고, 다시 $||\\mathbf w||^2/2$의 최솟값을 구하는 것과 같음  \n",
    "> * $||\\mathbf w||$는 $\\mathbf w=\\mathbf 0$에서 미분 불가능이지만, $||\\mathbf w||^2$은 모든 점에서 미분가능하므로 여러 최적화 알고리즘에서 더 잘 동작 \n",
    "\n",
    "\n",
    "* 위와 같이 선형적인 제약(부등식)이 있는 (아래로) 볼록한 이차 최적화문제를 **Quadratic programming(QP)** 문제라고 하며, QP 문제를 푸는 여러 알고리즘이 존재 (`ML2020-09-01-(컨벡스) 최적화 - 원시문제와 쌍대문제` 참고)  \n",
    "> * 힌지손실이라는 손실함수에 대해 서브그래디언트를 이용한 경사하강법을 적용할 수 있음 (아래 힌지손실함수 참고)  \n",
    "> * `sklearn.svm.LinearSVC`는 선형 SVM 분류 문제에 대한 최적화 알고리즘으로 Dual Cordinate Desecnt Method라고 부르는 알고리즘을 구현한 liblinear라이브러리를 사용  \n",
    "> * 비선형 SVM 분류 문제에 적용할 수 있는 `sklearn.svm.SVC`는 최적화 알고리즘으로 Sequential Minimal Optimization을 구현한 libsvm 라이브러리를 기반으로 함  \n",
    "\n",
    "\n",
    "* <span style=\"color:blue\">하드마진 선형 SVM 분류기를 학습시킨 후, 평면에 대한 마진 바깥부분(평면까지의 거리가 마진보다 큰)에 훈련 샘플을 추가하는 것은 SVM 분류기에 영향을 주지 않음  </span> (SVM 분류기에 영향을 주는 것은 서포트벡터)\n",
    "\n",
    "\n",
    "* <span style=\"color:red\"> SVM은 특성벡터의 스케일에 민감하므로, 학습을 시키기 전에 반드시 스케일링을 해주어야 함</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소프트마진 선형 SVM 분류 모델 \n",
    "\n",
    "* 하드마진 SVM 분류기에는 두 가지 문제점이 있음 \n",
    "> * 선형분리가능하지 않은 데이터셋에 대해서는 적용을 할 수 없다는 점 \n",
    "$$ $$\n",
    "> * 선형분리가능하더라도 이상치를 포함한 데이터셋에 대해서는 일반화가 잘 되지 않을 수 있음 \n",
    "<img src=\".\\images\\week11\\svm_c3.png\" width=\"400\">\n",
    "\n",
    "\n",
    "* 하드마진 SVM 분류기의 문제점을 해결하기 위해서는 좀 더 유연한 모델이 필요  \n",
    "> * 가능한 마진을 크게 잡는 것과 **마진오류(margin violation)** (샘플이 초평면과 마진사이에 포함되는 경우)를 최소화하는 것 사이에 적절한 균형을 잡아야 함 \n",
    "<img src=\".\\images\\week11\\svm_c4.png\" width=\"300\">\n",
    "\n",
    "\n",
    "* 마진을 크게 하는 것과 마진오류를 줄이는 것 사이의 균형을 잡기 위해 다음과 같은 최적화 문제(QP 문제)를 통해 초평면(즉, $\\mathbf w,\\, b$)를 구하는 것을 **소프트마진 선형 SVM 분류기**라고 함  \n",
    "[**원초문제**]\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\quad \\min_{\\mathbf w,b,\\boldsymbol{\\xi}} \\dfrac 1 2 ||\\mathbf w||^2+C \\sum_{i=1}^m \\xi_i\\\\\n",
    "&\\text{s.t }\\ y_i(\\langle \\mathbf w,\\mathbf x_i\\rangle +b)\\ge 1-\\xi_i,\\qquad (1\\le i \\le m)\\\\\n",
    "&\\quad \\ \\xi_i\\ge 0,\\qquad (1\\le i \\le m)\n",
    "\\end{aligned} \\quad \\cdots (2)\n",
    "$$ \n",
    "> * 위 최적화 문제의 제한조건에서 각 $\\xi_i\\ge 0$를 각 샘플에 대한 슬랙변수(slack variable)이라 하는데, $i$번째 샘플이 마진을 얼마나 위반할 지를 나타내는 변수 \n",
    "$$ $$\n",
    "> * 최적화에 이용되는 알고리즘은 위 하드마진 SVM 분류에 대한 설명 참고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM 분류의 최적화 문제에 대한  쌍대문제  \n",
    "\n",
    "* <span style=\"color:blue\">이 내용을 읽기 전에 `ML2020-09-01-(컨벡스) 최적화 - 원시문제와 쌍대문제` 내용을 복습한 후 공부할 것 </span>\n",
    "\n",
    "\n",
    "\n",
    "* 하드마진 SVM 최적화 문제와 소프트마진 SVM 최적화 문제 모두 목적함수(최소값을 구하는 대상이 되는 함수)와  부등식 제약조건의 함수가 아래로 볼록한 함수이므로 컨벡스 최적화 문제  \n",
    "\n",
    "\n",
    "* 또, 두 문제 모두 Slater 조건을 만족하므로, <span style=\"color:red\">주어진 문제(원초문제)에 대한 쌍대문제 사이에 강한 쌍대성이 성립하고 KKT 조건이 성립함 (원초문제의 최솟값과 쌍대문제의 최댓값이 같아짐)</span>   \n",
    "> * 따라서 주어진 SVM 분류와 쌍대문제에 대한 쌍대차이(dual gap)가 $0$이 되고, 쌍대문제를 해결한 후 KKT 조건을 이용하면 원초문제(SVM 분류에서의 최적화 문제)가 최소가 되는 파라미터를 구할 수 있음 \n",
    "\n",
    "\n",
    "* 실제 쌍대문제와 KKT 조건을 살펴보자.   \n",
    "> * 소프트마진 SVM 문제에 대한 라그랑주 함수는 라그랑주 승수(multiplier) $\\boldsymbol{\\alpha}=(\\alpha_1,\\cdots,\\alpha_m)\\succeq 0$, $\\boldsymbol{\\gamma}=(\\gamma_1,\\cdots,\\gamma_m)\\succeq 0$에 대해  \n",
    "$$\n",
    "L(\\mathbf w,b,\\boldsymbol{\\xi},\\boldsymbol{\\alpha},\\boldsymbol{\\gamma})=\\dfrac 1 2 ||\\mathbf w||^2+C\\sum_{i=1}^m \\xi_i -\\sum_{i=1}^m \\alpha_i\\bigl(y_i(\\langle \\mathbf w,\\mathbf x_i\\rangle+b)-1+\\xi_i\\bigr) -\\sum_{i=1}^m \\gamma_i \\xi_i\n",
    "$$\n",
    "> * 쌍대문제는  $\\boldsymbol{\\alpha}\\succeq 0$, $\\boldsymbol{\\gamma}\\succeq 0$에 대해 $g(\\boldsymbol{\\alpha},\\boldsymbol{\\gamma})=\\min_{\\mathbf w,b,\\boldsymbol{\\xi}}L(\\mathbf w,b,\\boldsymbol{\\xi},\\boldsymbol{\\alpha},\\boldsymbol{\\gamma})$의 최댓값을 구하는 문제  \n",
    ">> * $\\min_{\\mathbf w,b,\\boldsymbol{\\xi}}L(\\mathbf w,b,\\boldsymbol{\\xi},\\boldsymbol{\\alpha},\\boldsymbol{\\gamma})$를 구할 때, 라그랑주 함수가 $\\mathbf w,b,\\boldsymbol{\\xi}$에 대해 아래로 볼록한 함수이므로 그래디언트 벡터가 $0$이 되는 점 $\\mathbf w^*,b^*,\\boldsymbol{\\xi}^*$에서 최소가 되므로 다음이 성립  <span style=\"color:red\"> (check 1)</span>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial L}{\\partial \\mathbf w} \\Longrightarrow &\\, \\mathbf w^* =\\sum_{i=1}^m \\alpha_i y_i \\mathbf x_i\\\\ \n",
    "\\dfrac{\\partial L}{\\partial b} \\Longrightarrow&\\, \\sum_{i=1}^m \\alpha_i y_i=0\\\\\n",
    "\\dfrac{\\partial L}{\\partial \\xi_i}\\Longrightarrow&\\, C-\\alpha_i-\\gamma_i=0 \\ (1\\le i \\le m)\n",
    "\\end{aligned}\n",
    "$$  \n",
    "$$ $$\n",
    ">> * 이때, 쌍대문제의 목적함수는 $g(\\boldsymbol{\\alpha},\\boldsymbol{\\gamma})$는 다음과 같음 <span style=\"color:red\"> (check 2)</span>\n",
    "$$ \n",
    "g(\\boldsymbol{\\alpha},\\boldsymbol{\\gamma})= -\\dfrac 1 2 \\sum_{i=1}^m\\sum_{j=1}^m y_iy_j\\langle \\mathbf x_i,\\mathbf x_j\\rangle \\alpha_i\\alpha_j +\\sum_{i=1}^m \\alpha_i\n",
    "$$  \n",
    "$$ $$\n",
    ">> * 따라서, 소프트마진 SVM 최적화 문제에 대한 쌍대문제는 다음과 같음 <span style=\"color:red\"> (check 3)</span>  \n",
    "[**쌍대문제**]\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\quad \\min_{\\boldsymbol{\\alpha}} \\dfrac 1 2 \\sum_{i=1}^m\\sum_{j=1}^m y_iy_j\\langle \\mathbf x_i,\\mathbf x_j\\rangle \\alpha_i\\alpha_j -\\sum_{i=1}^m \\alpha_i\\\\\n",
    "&\\text{s.t }\\ 0\\le \\alpha_i\\le C,\\qquad (1\\le i \\le m)\\\\\n",
    "&\\quad \\sum_{i=1}^m y_i\\alpha_i=0 \n",
    "\\end{aligned} \\quad \\cdots (3)\n",
    "$$ \n",
    "> * 강한 쌍대성이 성립함을 알고 있으므로, 원초문제(SVM 분류 문제)가 $\\mathbf w^*, b^*,\\boldsymbol{\\xi}^*$에서 최솟값을 가지고, 쌍대문제는 $\\boldsymbol{\\alpha}^*,\\boldsymbol{\\gamma}^*$에서 최댓값을 가진다고 하면  \n",
    "다음 KKT 조건이 성립 <span style=\"color:red\"> (check 4)</span> \n",
    ">> * (primal feasibility 와 dual feasibility)\n",
    ">> * (stationary) 쌍대문제를 풀어 $\\boldsymbol{\\alpha}^*$를 구하면 $\\mathbf w^*$를 구할 수 있음 \n",
    "$$\n",
    "\\mathbf w^* =\\sum_{i=1}^m \\alpha_i^* y_i \\mathbf x_i,\\quad \n",
    "\\sum_{i=1}^m \\alpha_i^* y_i=0,\\quad \n",
    "C-\\alpha_i^*-\\gamma_i^*=0 \\ (1\\le i \\le m)\n",
    "$$\n",
    ">> <span style=\"color:blue\">이때, $\\alpha_i^*\\neq 0$인 $i$에 대응되는 $\\mathbf x_i$만이 $\\mathbf w^*$에 영향을 줄 수 있는 서포트벡터 </span>\n",
    "$$ $$\n",
    ">> * (complementary slackness) 모든 $1\\le i \\le m$에 대해 다음이 성립 \n",
    "$$ \\alpha_i^*y_i\\bigl(\\langle\\mathbf w^*,\\mathbf x_i\\rangle +b^*\\bigr) = \\alpha_i^*(1-\\xi_i^*),\\quad \\gamma_i^* \\xi_i^*=0$$\n",
    "$$ $$\n",
    ">> * <span style=\"color:blue\">$\\mathbf x_i$가 서포트벡터(즉, $\\alpha_i\\neq 0$)일 때, 다음이 성립 <span style=\"color:red\"> (check 5)</span>\n",
    ">>> * $0<\\alpha_i^*<C$인 경우 $\\gamma_i^*\\neq 0$이므로 $\\xi_i^*= 0$이 되어 $\\mathbf x_i$는 마진의 경계에 위치하고  $y_i\\bigl(\\langle\\mathbf w^*,\\mathbf x_i\\rangle +b^*\\bigr)=1$로부터 $b^*$를 구할 수 있음  </span> \n",
    "\n",
    "\n",
    "* 소프트마진 SVM 분류 문제와 비교하여 <span style=\"color:blue\">쌍대문제는 특성벡터의 차원(특성 개수)에는 상관없고, 훈련 데이터 샘플의 개수에 의존함:   \n",
    "따라서, 훈련 샘플 수가 특성의 개수보다 작을 때(예를 들어, 게놈 데이터) 원초문제를 푸는 것보다 쌍대문제를 푸는 것이 더 빠름  </span>  \n",
    "\n",
    "* SVM 분류 문제의 쌍대문제에 대한 장점: <span style=\"color:blue\"> 원초문제와 달리 쌍대문제의 목적함수는 $\\mathbf x_i$ 사이의 내적(유클리드 내적)을 이용하여 표현되어 있으므로 이 내적을 일반적인 내적으로 확장하여 비선형문제에도 적용시킬 수 있음 (아래 비선형 SVM 분류참고)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM 분류에서의 최적화 문제를 SVM 분류기에 대한 손실함수 관점에서 해석  \n",
    "\n",
    "* 힌지손실(hinge loss)함수 $\\max\\{0,1-t\\}$: $t=1$을 제외하면 미분가능하고 아래로 볼록한 함수($t=1$에서의 서브그래디언트벡터는 $[-1,0]$)이므로 서브그래디언트 벡터를 이용한 경사하강법이 가능 <img src=\".\\images\\week11\\hinge.png\" width=\"200\">\n",
    "\n",
    "* 힌지손실(hinge loss)함수에 대한 최솟값을 구하는 문제 $\\min_t\\max\\{0,1-t\\}$는 다음 최적화 문제와 동치임 \n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_{\\xi, t}\\xi \\\\\n",
    "\\text{s.t } & \\xi \\ge 0\\\\\n",
    "& \\xi \\ge 1-t\n",
    "\\end{aligned} \\ \n",
    "$$\n",
    "\n",
    "\n",
    "* 따라서 소프트마진 SVM 문제(2)는 다음과 같은 손실함수를 갖는 모델로 생각할 수 있음  \n",
    "> * 소프트마진 SVM 분류기의 손실함수 \n",
    "$$ J(\\mathbf w,b)=\\dfrac 1 2 ||\\mathbf w||_2^2 +C\\sum_{i=1}^m \\max\\{0,1-y_i(\\langle \\mathbf w,\\mathbf x_i\\rangle+b)\\}\\ \\cdots (4)$$  \n",
    "$$ $$\n",
    "> * 힌지손실함수 입장에서 보면 $||\\mathbf w||^2$은 규제 항(패널티 항)으로 볼 수 있으므로, $C$는 릿지회귀에서의 규제 하이퍼파라미터 $\\alpha$와 비교하여 역수의 역할을 함을 이해할 수 있음  \n",
    ">> * SVM 모델이 과대적합이라면 $C$를 감소시켜 모델을 규제할 수 있음 \n",
    "\n",
    "* 즉, SVM 분류문제에서 마진을 최대화하는 것을 힌지손실함수를 통해 해석하면 규제를 가하는 것으로 해석할 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비선형 SVM 분류과 커널(kernel) SVM 분류 모델 \n",
    "\n",
    "### 1. 비선형 SVM 분류 \n",
    "\n",
    "* 선형적으로 분류할 수 없는 비선형 데이터셋이 주어질 때, 선형회귀 모델에서 다루었던 것처럼 다항 특성 또는 다른 비선형 특성을 추가한 후 선형 SVM 모델을 학습시키는 것이 가능   \n",
    "\n",
    "<img src=\".\\images\\week11\\nonlinear.png\" width=\"500\">\n",
    "\n",
    "* 다항식 특성 등을 추가하는 것은 간단하지만, 낮은 차수의 다항식은 아주 복잡한 데이터셋을 잘 표현하지 못하고 높은 차수의 다항식 특성은 굉장히 많은 특성을 추가해야 하므로 모델을 학습시키는데 시간이 걸림 (원래 속성이 $n$개일 때, $d$차 다항식 특성의 개수는 $\\dfrac{(n+d)!}{n!d!}$개)\n",
    "\n",
    "### 2. 커널과 커널 트릭 \n",
    "\n",
    "* <span style=\"color:blue\">하지만, SVM 분류의 학습 문제와 동등한 쌍대문제(식 (3))을 보면, 실제로 계산에 필요한 것은 특성벡터 사이의 내적 $\\langle \\mathbf x_i,\\mathbf x_j\\rangle$이 중요하며, 이 내적을 특성벡터의 차원과 상관없이 효율적으로 계산할 수 있으면 앞에서 언급한 특성 추가시의 문제점을 해결할 수 있음 </span> \n",
    "> * 예를 들어, 주어진 샘플 벡터가 $\\mathbf x = (x_1,x_2)^{\\rm T}$일 때, 이차다항식 특성벡터를 $\\phi(\\mathbf x)=(x_1^2,x_2^2,\\sqrt 2 x_1x_2,\\sqrt 2 x_1,\\sqrt 2 x_2,1)^{\\rm T}$와 같이 생성하고, 새로 얻은 특성벡터 사이의 내적을 계산하면 (단, $\\mathbf x_i = (x_{i1},x_{i2})^{\\rm T})$\n",
    "$$ $$\n",
    "$$\\langle \\phi(\\mathbf x_i),\\phi(\\mathbf x_j)\\rangle = x_{i1}^2x_{j1}^2+x_{i2}^2x_{j2}^2+2x_{i1}x_{i2}x_{j1}x_{j2}+2x_{i1}x_{j1}+2x_{i2}x_{j2}+1= (\\langle\\mathbf x_i,\\mathbf x_j\\rangle +1)^2$$   \n",
    ">> * $\\mathbf x_i$로부터 새로운 특성벡터를 생성하는 함수를 **특성맵(feature map)**이라고 함 \n",
    "> * 위의 예에서, (편차제외하고 5차원 벡터) $\\phi(\\mathbf x_i)$와 $\\phi(\\mathbf x_j)$를 직접 구하지 않더라도(즉, 특성맵을 구하지 않더라도) 2차원 벡터에 대한 $\\langle\\mathbf x_i,\\mathbf x_j\\rangle$의 함수 $K(\\mathbf x_i,\\mathbf x_j):=(\\langle \\mathbf x_i,\\mathbf x_j\\rangle+1)^2$을 이용하면 식 (3)에서 특성벡터 $\\phi(\\mathbf x_i)$에 대한 SVM을 적용시킬 수 있음  \n",
    "$$ $$\n",
    "> * 일반적으로 $d$차 다항 특성벡터에 대한 대한 내적 $\\langle\\phi(\\mathbf x_i),\\phi(\\mathbf x_j)\\rangle$는 $\\phi(\\mathbf x_i)$를 구하지 않더라도 $K(\\mathbf x_i,\\mathbf x_j):=(\\langle \\mathbf x_i,\\mathbf x_j\\rangle+1)^d$와 같이 계산할 수 있음  \n",
    ">> * 이때, 함수 $K(\\mathbf x_i,\\mathbf x_j):=(\\langle \\mathbf x_i,\\mathbf x_j\\rangle+1)^d$를 **$d$차 다항식 커널(kernel)**이라고 함 \n",
    "$$ $$\n",
    ">> * 이처럼 커널을 이용하여 특성벡터를 직접 구하지 않고, SVM 분류기를 학습시키는 방법을 **커널 트릭(kernel trick)**이라고 함 \n",
    "\n",
    "### 3. 일반적인 커널 \n",
    "\n",
    "* 일반적으로 연속함수 $K:\\mathbb R^n\\times \\mathbb R^n\\to \\mathbb R$에 대해 다음 두 조건을 만족할 때 **커널**이라고 함   \n",
    "> (조건 1) 임의의 두 벡터 $\\mathbf x,\\mathbf z\\in \\mathbb R^n$에 대해  $K(\\mathbf x, \\mathbf z) = K(\\mathbf z, \\mathbf x)$  \n",
    "$$ $$\n",
    "> (조건 2) 모든 자연수 $m$에 대해, $m$개의 벡터 $\\mathbf x_1,\\cdots,\\mathbf x_m$와 $m$개의 실수 $c_1,\\cdots,c_m$이 임의로 주어질 때,  \n",
    "$\\displaystyle{\\sum_{i=1}^m\\sum_{j=1}^m K(\\mathbf x_i,\\mathbf x_j)c_ic_j\\ge 0}$  \n",
    "\n",
    "\n",
    "* $K:\\mathbb R^n\\times \\mathbb R^n\\to \\mathbb R$가 위 조건을 만족하는 커널이면 \n",
    "적당한 힐버트 공간 $\\mathcal H$와 특성함수 $\\phi:\\mathbb R^n\\to \\mathcal H$가 존재해서  \n",
    "$$ $$\n",
    "$$K(\\mathbf x,\\mathbf z)=\\langle \\phi(\\mathbf x),\\phi(\\mathbf z)\\rangle_{\\mathcal H}$$  \n",
    "가 성립함을 보일 수 있고, SVM에 적용할 수 있음 (힐버트 공간이란 완비성을 만족하는 내적공간)  \n",
    "\n",
    "\n",
    "* 보다 일반적인 상황에서 커널을 다룰 수 있지만, 이 강의의 수준을 넘어섬 (참고 [Mercer의 정리](https://en.wikipedia.org/wiki/Mercer%27s_theorem))\n",
    "\n",
    "### 4. 커널 SVM \n",
    "\n",
    "* SVM에서 특성벡터를 직접 구하는 대신 커널 $K$에 대한 커널 트릭을 사용하여 다음과 같이 SVM 최적화 문제의 쌍대문제를 해결하고 이를 통해 SVM 분류기 예측을 수행하는 모델을 **커널 SVM** 모델이라고 함   \n",
    "$$ $$\n",
    "[**커널 SVM 쌍대문제**]\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\quad \\min_{\\boldsymbol{\\alpha}} \\dfrac 1 2 \\sum_{i=1}^m\\sum_{j=1}^m y_iy_j K(\\mathbf x_i,\\mathbf x_j) \\alpha_i\\alpha_j -\\sum_{i=1}^m \\alpha_i\\\\\n",
    "&\\text{s.t }\\ 0\\le \\alpha_i\\le C,\\qquad (1\\le i \\le m)\\\\\n",
    "&\\quad \\sum_{i=1}^m y_i\\alpha_i=0 \n",
    "\\end{aligned} \\quad \\cdots (5)\n",
    "$$ \n",
    "\n",
    "\n",
    "* SVM에서 자주 사용하는 커널은 다음과 같음  \n",
    "> * 선형 커널 : $K(\\mathbf x,\\mathbf z)=\\mathbf x^{\\rm T}\\mathbf z$  \n",
    "$$ $$\n",
    "> * $d$차 다항식 커널 : $K(\\mathbf x,\\mathbf z)=(\\gamma \\mathbf x^{\\rm T}\\mathbf z +r)^d$, $(\\gamma,\\, r\\in \\mathbb R)$  \n",
    "$$ $$\n",
    "> * 가우시안 RBF(Radial Basis Function) : $K(\\mathbf x,\\mathbf z)=\\exp(-\\gamma||\\mathbf x-\\mathbf z||^2)$ \n",
    "$$ $$\n",
    "> * 시그모이드 : $K(\\mathbf x,\\mathbf z)=\\text{tanh}(\\gamma \\mathbf x^{\\rm T}\\mathbf z + r)$\n",
    "\n",
    "* <span style=\"color:red\">커널 SVM에서 예측을 하는 방법</span>: <span style=\"color:blue\">편차 $b^*$는 구할 수 있지만 모델 파라미터 $\\mathbf w^*$는 직접 구할 수 없는 상황 </span>\n",
    "> * 커널 SVM에서 학습 알고리즘은 선형 SVM의 쌍대문제에서 특성 맵 $\\phi$을 통해 구체적인 특성벡터 $\\phi(\\mathbf x)$사이의 내적을 계산하는 대신 커널 트릭을 이용하는 것이므로 최적해를 주는 $\\alpha^*$를 구할 수 있지만, 이로부터 원초문제의 $\\mathbf w^*$를 구하면 <span style=\"color:red\">(check 4)</span>에 의해 \n",
    "$$\\mathbf w^* = \\sum_{i=1}^m \\alpha_i^*y_i\\phi(\\mathbf x_i)$$\n",
    "가 되어 특성 맵을 모르면 $\\mathbf w^*$를 구할 수 없다. (실제로 $\\mathbf w^*$는 특성벡터 $\\phi(\\mathbf x_i)$와 같은 차원이어야 함)\n",
    "$$ $$\n",
    "> * 하지만, <span style=\"color:blue\">주어진 샘플 벡터 $\\mathbf x$에 대한 예측은 \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\langle \\mathbf w^*,\\phi(\\mathbf x)\\rangle+b^*= &\\langle \\bigl(\\sum_{i=1}^m \\alpha^*y_i\\phi(\\mathbf x_i)\\bigr),\\phi(\\mathbf x)\\rangle+b^*\\\\\n",
    "=& \\sum_{i=1}^m  \\alpha^*y_i \\langle \\phi(\\mathbf x_i),\\phi(\\mathbf x)\\rangle +b^*\\\\\n",
    "=& \\sum_{i=1}^m  \\alpha^*y_i K(\\mathbf x_i,\\mathbf x) +b^*\n",
    "\\end{aligned}\n",
    "$$</span> \n",
    "이므로 상수 $b^*$만 구할 수 있으면 $\\mathbf w^*$를 직접 구하지 않고도 예측을 할 수 있음  \n",
    "$$ $$ \n",
    "> * 실제로 편차 $b^*$는 <span style=\"color:red\">(check 5)</span>에 의해 $0<\\alpha_i^*<C$인 $i$에 대해 \n",
    "$$y_i\\bigl(\\langle \\mathbf w^*,\\phi(\\mathbf x_i)\\rangle+b^*\\bigr)=1$$\n",
    "$$ $$\n",
    "이 성립하므로 $\\{i|1\\le i\\le m, 0<\\alpha_i^*<C\\}$의 원소의 개수를 $n_s$라 할 때, \n",
    "$$ \\sum_{\\substack{i=1\\\\ 0<\\alpha^*_i<C}}^m \\bigl(y_i-\\langle \\mathbf w^*,\\phi(\\mathbf x_i)\\rangle\\bigr) = n_s b^*$$\n",
    "$$ $$\n",
    "가 되고, 이로부터 <span style=\"color:blue\">다음과 같이 $b^*$를 구할 수 있음 </span>\n",
    "$$ b^* = \\dfrac 1{n_s} \\sum_{\\substack{i=1\\\\0<\\alpha_i^*<C}}^m\\left(y_i - \\sum_{j=1}^m \\alpha_j^* y_j K(\\mathbf x_j,\\mathbf x_i)\\right)\n",
    "$$\n",
    "(위 등식에서 $\\displaystyle{\\sum_{j=1}^m}$은 $\\displaystyle{\\sum_{\\substack{j=1\\\\0<\\alpha_j^*\\le C}}^m}$과 같아짐에 주목) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서포트벡터머신 분류 모델 (sklearn의 사용가능한 클래스)\n",
    "\n",
    "* 소프트마진 선형 SVM 분류기에 대한 `sklearn`의 클래스들  \n",
    "\n",
    "> 1] 손실함수 (식 (5))의 최솟값을 효율적으로 구하는 최적화 알고리즘(liblinear 라이브러리)을 구현한 `sklearn.svm` 모듈의 `LinearSVC` 클래스 ([API](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html?highlight=linearsvc#sklearn.svm.LinearSVC))  \n",
    ">> * 커널 트릭을 지원하지 않지만, 훈련 시간 복잡도는 샘플 수 $m$과 특성 수 $n$에 대해 대략 $O(m\\times n)$  \n",
    "$$ $$\n",
    ">> * <span style=\"color:blue\">`LinearSVC`는 규제에 편향을 포함시켜 구현되어 있으므로, 훈련 데이터셋에서 평균을 빼서 변형한 데이터셋의 평균이 $0$이 되도록 조절해야 함 </span>   \n",
    "(보통 `StandardScaler`를 통해 스케일링을 해야 하므로 이 문제는 해결됨)  \n",
    "$$ $$\n",
    ">> * <span style=\"color:blue\">`loss`의 기본값이 \"squared_hinge\"로 설정되어 있으므로 `loss=\"hinge\"`로 설정해주어야 함  </span>\n",
    "$$ $$\n",
    ">> * <span style=\"color:blue\">훈련 샘플보다 특성의 개수가 많으면 `dual=True`로 설정하여 쌍대문제를 풀고, 훈련 샘플이 더 많은 경우는 `dual=False`로 설정하여 성능을 조절 </span>\n",
    "\n",
    "> 2] 손실함수 (식 (5))의 최솟값을 구하기 위해 서브그래디언트에 대한 경사하강법을 사용하는 `sklearn.linear_model` 모듈의 `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`클래스 ([API](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html?highlight=sgdclassifier#sklearn.linear_model.SGDClassifier))\n",
    ">> * 손실함수를 \"hinge\"로 설정하고 규제 하이퍼파라미터는 `alpha`는 `C`의 역수이므로 위와 같이 설정(m은 샘플 수)\n",
    "$$ $$\n",
    ">> * `LinearSVC` 보다 빠르지 않지만, 데이터셋이 아주 커서 메모리에 적재할 수 없거나, 온라인 학습으로 분류문제를 다룰 때 유용 \n",
    "$$ $$\n",
    ">> * 이 경우에도 스케일링이 필요함 \n",
    "\n",
    "> 3] 커널 트릭을 이용하여 SVM 분류에 대한 쌍대문제를 푸는 알고리즘(libsvm 라이브러리 기반)을 구현한 `sklearn.svm` 모듈의 `SVC` 클래스를 이용하여 `SVC(kernel=\"linear\")`로 모델 객체를 생성 ([API](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC))  \n",
    ">> * 훈련 시간의 복잡도는  샘플 수 $m$과 특성 수 $n$에 대해 대략 $O(m^2\\times n)$과 $O(m^3\\times n)$사이  \n",
    "$$ $$\n",
    ">> * 다항식 커널 $K(\\mathbf x,\\mathbf z)=(\\gamma \\mathbf x^{\\rm T}\\mathbf z +r)^d$을 사용하는 커널 SVM 분류기 모델의 대표적인 예 :  \n",
    "`SVC(kernel=\"poly\", degree=3, gamma=\"scale\", coef0=1, C=5)`와 같은 형태로 객체 생성   \n",
    "(`degree`,`gamma`,`coef0`는 각각 커널식에서 $d$, $\\gamma$, $r$에 대응되는 입력변수) [API 참고]\n",
    "$$ $$\n",
    ">> * 가우시안 RBF 커널 $K(\\mathbf x,\\mathbf z)=\\exp(-\\gamma||\\mathbf x-\\mathbf z||^2)$을 사용하는 커널 SVM 분류기 모델 :   \n",
    "`SVC(kernel=\"rbf\", gamma=5, C=5)`와 같은 형태로 객체 생성   \n",
    "(`gamma`는 커널식에서 $\\gamma$에 대응되는 입력변수) [API 참고]  \n",
    "$$ $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서포트벡터머신 회귀 \n",
    "\n",
    "* 서포트벡터머신은 선형, 비선형 분류 뿐만 아니라 선형, 비선형 회귀 문제에도 이용할 수 있음  \n",
    "\n",
    "\n",
    "* $\\epsilon>0$과 훈련 데이터셋 $\\{(\\mathbf x_1,y_1),\\cdots,(\\mathbf x_m,y_m)\\}$이 주어질 때, 선형 서포트벡터머신 회귀는 선형함수 $f(\\mathbf x)=\\langle \\mathbf w,\\mathbf x\\rangle +b$ 중에서 $|y_i-f(\\mathbf x_i)|\\le \\epsilon, (1\\le i\\le m)$이 되는 $\\mathbf w,b$를 구하는 모델 (여기서 $\\epsilon$은 마진에 해당)\n",
    "> * 위에서 기술한 목표는 **하드마진 SVM 회귀문제**에 해당 \n",
    "$$ $$\n",
    "> * 실제로는 위 조건(모든 $i=1,\\cdots,m$)을 만족하는 $f(\\mathbf x)$를 구할 수 없는 경우가 있으므로 **소프트마진 SVM 회귀문제**는 조건을 완하하여 \n",
    "$$  $$\n",
    "$$-\\epsilon-\\xi_i' \\le y_i-(\\langle \\mathbf w,\\mathbf x_i\\rangle+b)\\le \\epsilon+\\xi_i, \\quad (\\forall\\, 1\\le i\\le m,\\ \\xi_i\\ge 0,\\, \\xi_i'\\ge 0)$$\n",
    "$$ $$\n",
    "을 만족하는 $\\mathbf w,b$를 찾는 문제를 생각 ($\\xi_i$와 $\\xi_i'$은 슬랙변수)\n",
    "$$ $$\n",
    "> * 즉, 분류 문제와는 반대로 제한된 마진오류 내에서 마진 경계사이에 최대한 많은 샘플이 포함되도록 학습  \n",
    "$$ $$\n",
    "> * 소프트마진 SVM 회귀문제를 규제항을 포함하여 힌지손실함수로 나타내면  다음과 같음 \n",
    "$$ \n",
    "\\min_{\\mathbf w,b}\\left(\\dfrac 1 2 ||\\mathbf w||^2 + C\\sum_{i=1}^m \\max\\bigl(0, |y_i-\\langle \\mathbf w,\\mathbf x_i\\rangle|-\\epsilon\\bigr)\\right)\\quad \\cdots (6)\n",
    "$$\n",
    "$$ $$\n",
    ">* 위 손실함수를 최소화하는 것을 소프트마진 SVM 회귀 문제로 표현하면 \n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{\\mathbf w,b,\\boldsymbol{\\xi},\\boldsymbol{\\xi'}} \\dfrac 1 2 ||\\mathbf w||^2 +C\\sum_{i=1}^m\\bigl(\\xi_i+\\xi_i'\\bigr)\\\\\n",
    "\\text{s.t }&\\  y_i -\\langle \\mathbf w,\\mathbf x_i\\rangle-b \\le \\xi_i,\\quad (1\\le i \\le m)\\\\\n",
    "&\\ \\langle  \\mathbf w,\\mathbf x_i\\rangle+b-y_i \\le \\xi_i', \\quad (1\\le i \\le m)\\\\\n",
    "&\\ \\xi_i\\ge 0, \\quad (1\\le i \\le m)\\\\\n",
    "&\\ \\xi_i'\\ge 0, \\quad (1\\le i \\le m)\\\\\n",
    "\\end{aligned}\\quad \\cdots (7)\n",
    "$$\n",
    "$$ $$\n",
    "> * 위 원초문제도 컨벡스 최적화 문제이고 Slater 조건을 만족하므로 강한 쌍대성이 만족되고, 쌍대문제를 구하고 나면 소프트마진 SVM 분류의 경우처럼 커널 트릭을 이용한 커널 SVM 회귀모델을 구성하는 것도 가능 (보다 자세한 내용은 [링크를 참고](https://scikit-learn.org/stable/modules/svm.html#svm-regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서포트벡터머신 회귀 모델 (sklearn의 사용가능한 클래스) \n",
    "\n",
    "* SVM 분류기를 생성하는 `LinearSVC` 클래스의 회귀 버전에 해당하는 `sklearn.svm` 모듈의 `LinearSVR` 클래스 ([API](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html?highlight=linearsvr#sklearn.svm.LinearSVR))\n",
    "\n",
    "* 커널 SVM 분류기를 생성하는 `SVC` 클래스의 회귀 버전에 해당하는 `sklearn.svm` 모듈의 `SVR` 클래스 ([API](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html?highlight=svr#sklearn.svm.SVR))\n",
    "\n",
    "* `LinearSVR`은 훈련 데이터셋의 크기에 비례해서 선형적으로 학습 시간이 늘어나지만, `SVR`은 훨씬 느려짐에 유의 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이론 연습문제 \n",
    "\n",
    "1) <span style=\"color:red\"> (check 1)</span>을 증명하시오. \n",
    "\n",
    "\n",
    "2) <span style=\"color:red\"> (check 2)</span>를 증명하시오. \n",
    "\n",
    "\n",
    "3) <span style=\"color:red\"> (check 3)</span>을 증명하시오. \n",
    "\n",
    "\n",
    "4) <span style=\"color:red\"> (check 4)</span>를 증명하시오. \n",
    "\n",
    "\n",
    "5) <span style=\"color:red\"> (check 5)</span>를 증명하시오. \n",
    "\n",
    "6) 수백개의 특성을 가진 샘플 수백만 개를 이용하여 SVM 분류 문제를 학습시킬 때, 원초문제와 쌍대문제 중 어떤 것을 사용하는 것이 좋은지에 대해 설명하시오. \n",
    "\n",
    "7) 가우시안 RBF 커널의 특별한 경우($n=1$인 경우)로 $K(x,z)=\\exp(-\\gamma |x-z|^2)$일 때, $K(x,z)=\\phi(x)^{\\rm T}\\phi(x)$를 만족시키는 $\\phi:\\mathbb R \\to \\mathcal H$를 구하시오.  \n",
    "(단, $\\mathcal H=\\{(a_1,\\cdots,a_n,\\cdots)^{\\rm T}: \\sum_{i=1}^{\\infty}a_i^2\\in \\mathbb R\\}$)  \n",
    "(힌트: $\\exp(x)=\\sum_{n=0}^{\\infty}\\dfrac{x^n}{n!}$임을 이용. [일반적인 경우에 대한 자료 링크](https://en.wikipedia.org/wiki/Radial_basis_function_kernel))\n",
    "\n",
    "8) RBF 커널을 사용해 SVC 분류기를 훈련시킬 때, $\\gamma$ 값과 과대적합, 과소적합의 관계를 알아보시오. ([참고 자료 링크](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py)). 만약 생성된 모델이 훈련 데이터셋에 과소적합되었다면 $\\gamma$ 값을 증가시켜야 하는지, 감소시켜야 하는지에 대해 설명하시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
